{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_on_puddle_world",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASqF13XmDW4H"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyyZqiFovJaL"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16FoIkD0DakP"
      },
      "source": [
        "# Environment Description\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqN2raNqvR81"
      },
      "source": [
        "# This class is a representation of the puddle world and its properties.\n",
        "\n",
        "class PuddleEnv:\n",
        "    \n",
        "    def __init__(self,goal_letter):\n",
        "\n",
        "      #The reward matrix is a 12x12 grid which holds rewards for each state\n",
        "      self.reward_matrix = np.zeros((12,12))\n",
        "      self.fillRewardMatrix()\n",
        "\n",
        "      #The state state is assigned randomly from four options\n",
        "      self.startState = np.zeros((1,2))\n",
        "      self.assignStartState()\n",
        "      \n",
        "      #Different puddle worlds ('A','B' or 'C') have different goals\n",
        "      self.goal = np.array([0,0])\n",
        "      self.setGoal(goal_letter)\n",
        "      self.generalActions = np.array([[-1,0],[1,0],[0,-1],[0,1]])\n",
        "\n",
        "    def assignStartState(self):\n",
        "      '''\n",
        "      This function initiates the initial state one of the start states of the\n",
        "      Start states mentioned in the graph with equal probability\n",
        "      '''\n",
        "\n",
        "      random_number = random.random()\n",
        "      self.startState = np.zeros((1,2))\n",
        "\n",
        "      if random_number < 1/4:\n",
        "        self.startState = np.array([5,0])\n",
        "      elif random_number < 2/4:\n",
        "        self.startState = np.array([6,0])\n",
        "      elif random_number < 3/4:\n",
        "        self.startState = np.array([10,0])\n",
        "      else:\n",
        "        self.startState = np.array([11,0])\n",
        "\n",
        "\n",
        "    def returnReward(self,state):\n",
        "      '''\n",
        "      Returns the reward for a state reached in the puddle world\n",
        "      '''\n",
        "      return self.reward_matrix[state[0],state[1]]\n",
        "\n",
        "    def fillRewardMatrix(self):\n",
        "      '''\n",
        "      This function populates the reward matrix with values according \n",
        "      to the puddle world problem statement\n",
        "      '''\n",
        "\n",
        "      # Reward -1 states\n",
        "      for i in range(3,9):\n",
        "        self.reward_matrix[2,i] = -1;\n",
        "\n",
        "      for i in range(3,7):\n",
        "        self.reward_matrix[i,3] = -1;\n",
        "        self.reward_matrix[i,8] = -1;\n",
        "\n",
        "      for i in range(3,8):\n",
        "        self.reward_matrix[8,i] = -1;\n",
        "\n",
        "      self.reward_matrix[6,7] = -1;\n",
        "      self.reward_matrix[7,7] = -1;\n",
        "      self.reward_matrix[7,3] = -1;\n",
        "\n",
        "      # Reward -2 states\n",
        "      for i in range(4,8):\n",
        "        self.reward_matrix[3,i] = -2;\n",
        "\n",
        "      for i in range(4,6):\n",
        "        self.reward_matrix[i,4] = -2;\n",
        "        self.reward_matrix[i,7] = -2;\n",
        "\n",
        "      for i in range(4,7):\n",
        "        self.reward_matrix[7,i] = -2;\n",
        "      \n",
        "      self.reward_matrix[5,6] = -2;\n",
        "      self.reward_matrix[6,4] = -2;\n",
        "      self.reward_matrix[6,6] = -2;\n",
        "\n",
        "      # Reward -3 states\n",
        "      self.reward_matrix[4,5] = -3;\n",
        "      self.reward_matrix[4,6] = -3;\n",
        "      self.reward_matrix[5,5] = -3;\n",
        "      self.reward_matrix[6,5] = -3;\n",
        "\n",
        "    def setGoal(self,goal_character):\n",
        "      '''\n",
        "      This function assigns the goal state depending on the \n",
        "      vairant of the puddle problem chosen\n",
        "      '''\n",
        "\n",
        "      if goal_character == 'A':\n",
        "        self.reward_matrix[0,11] = 10\n",
        "        self.goal = np.array([0,11])\n",
        "      if goal_character == 'B':\n",
        "        self.reward_matrix[2,9] = 10\n",
        "        self.goal = np.array([2,9])\n",
        "      if goal_character == 'C':\n",
        "        self.reward_matrix[6,7] = 10\n",
        "        self.goal = np.array([6,7])\n",
        "\n",
        "    def nextState(self,state,action):\n",
        "      '''\n",
        "      For a given state and action, this function takes into consideration\n",
        "      the stochasticity of the world and calculates a final state\n",
        "      '''\n",
        "\n",
        "      #possible_actions = [up:indx=0,  down:indx=1,  left:indx=2,  right:indx=3]\n",
        "      possible_actions = np.array([[-1,0],[1,0],[0,-1],[0,1]])\n",
        "\n",
        "      #Updating possible actions depending on the current state\n",
        "      #This specifically tackles the states which are in the edes of the map\n",
        "      if state[1] == 11:\n",
        "        possible_actions = np.delete(possible_actions,3,0)\n",
        "\n",
        "      if state[1] == 0:\n",
        "        possible_actions = np.delete(possible_actions,2,0)\n",
        "      \n",
        "      if state[0] == 11:\n",
        "        possible_actions = np.delete(possible_actions,1,0)\n",
        "      \n",
        "      if state[0] == 0:\n",
        "        possible_actions = np.delete(possible_actions,0,0)\n",
        "\n",
        "\n",
        "      #If the action cannot be taken at a particular state,\n",
        "      #We ignore that action and give possible action equal probabilities\n",
        "      flag = 0\n",
        "\n",
        "      for i in range(len(possible_actions)):\n",
        "        comparison = possible_actions[i] == action\n",
        "        if comparison.all():\n",
        "          action_index = i\n",
        "          flag = 1\n",
        "      \n",
        "\n",
        "      remaining_actions = np.delete(possible_actions,i,0)\n",
        "      newState = np.array([0,0])\n",
        "\n",
        "      if flag == 0:\n",
        "        random_number = random.random()\n",
        "        for i in range(len(remaining_actions)):\n",
        "          if random_number <= (i+1)/len(remaining_actions) and random_number >= i/len(remaining_actions):\n",
        "            newState = state + remaining_actions[i]\n",
        "  \n",
        "      #Determining what the new state will be,\n",
        "      #We shall calculate one effective action based on probabilities\n",
        "      else:\n",
        "        random_number = random.random()\n",
        "        if random_number <= 0.9:\n",
        "          newState = state + action\n",
        "\n",
        "        else:\n",
        "          for i in range(len(remaining_actions)):\n",
        "            if random_number <= 0.9 + (i+1)*0.1/len(remaining_actions) and random_number >= 0.9 + i*0.1/len(remaining_actions):\n",
        "              newState = state + remaining_actions[i]\n",
        "\n",
        "      \n",
        "      #Accounting for the westerly wind, with a probability of 0.5\n",
        "      #The new state might slide to the east by one square\n",
        "\n",
        "      random_number = random.random()\n",
        "\n",
        "      if random_number >= 0.5:\n",
        "        newState[1] += 1\n",
        "        if newState[1] == 12:\n",
        "          newState[1] = 11\n",
        "\n",
        "      return newState\n",
        "\n",
        "    def calculateTransition(self,state,action):\n",
        "      '''\n",
        "      This returns a transition vector which is of the form\n",
        "      [state,action,next_state,reward,done]\n",
        "      At a given state, and taking a certain action,\n",
        "      it adds the next state, reward as well as a boolean if the\n",
        "      final state has reached the goal\n",
        "      '''\n",
        "      newState = self.nextState(state,action)\n",
        "      reward_temp = self.returnReward(newState)\n",
        "      tempBool = False\n",
        "\n",
        "      comparison = newState == self.goal\n",
        "      if comparison.all():\n",
        "        tempBool = True\n",
        "      transition = [[state],[action],[newState],[reward_temp],[tempBool]]\n",
        "      return transition\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP6PXIPqvViN"
      },
      "source": [
        "class DQNAgent:\n",
        "    def create_model(self):\n",
        "        '''\n",
        "        The neural network we are using here is a three hidden layer \n",
        "        network with 32,64 and 16 hidden nodes in each of these layers.\n",
        "        It has an output layer with 4 linear \n",
        "        '''\n",
        "        model = Sequential()\n",
        "        model.add(Dense(32, input_shape=(1,2)))\n",
        "        model.add(Dense(64))\n",
        "        model.add(Dense(16))\n",
        "        model.add(Dense(4, activation='linear')) \n",
        "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Main model\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Target network\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        # An array with last n steps for training\n",
        "        self.replay_memory = deque(maxlen=10_000)\n",
        "\n",
        "        # An array with actions that the agent can take\n",
        "        self.generalActions = np.array([[-1,0],[1,0],[0,-1],[0,1]])\n",
        "\n",
        "        \n",
        "\n",
        "    # Adds step's data to a memory replay array\n",
        "    # (observation space, action, reward, new observation space, done)\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    def get_q(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1,1,2))\n",
        "\n",
        "    def model_next_action(self,state):\n",
        "      tempQValues = dqnagent.get_q(state.reshape(1,1,2))\n",
        "      bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "      action = np.array(self.generalActions[bestActionIndex])\n",
        "      return action \n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "      if len(self.replay_memory) < 32:\n",
        "            return\n",
        "\n",
        "      minibatch = random.sample(dqnagent.replay_memory, 32)\n",
        "\n",
        "      current_states = np.array([np.array(transition[0]).flatten() for transition in minibatch])\n",
        "      current_qs_list = dqnagent.model.predict(current_states.reshape(len(minibatch),1,2))\n",
        "     \n",
        "      new_current_states = np.array([np.array(transition[2]).flatten() for transition in minibatch])\n",
        "      future_qs_list = self.target_model.predict(new_current_states.reshape(len(minibatch),1,2))\n",
        "\n",
        "      X = []\n",
        "      y = []\n",
        "\n",
        "      for index, (current_state, action, new_current_state,reward, done) in enumerate(minibatch):\n",
        "        if not [done]:\n",
        "          max_future_q = np.max(future_qs_list[index])\n",
        "          new_q = reward + DISCOUNT * max_future_q\n",
        "        else:\n",
        "          new_q = reward\n",
        "\n",
        "  # Update Q value for given state\n",
        "        current_qs = np.array(current_qs_list[index]).flatten()\n",
        "        for k in range(4):\n",
        "          comparison = puddle.generalActions[k] == action\n",
        "          if comparison.all():\n",
        "            action_index = k\n",
        "        current_qs[action_index] = new_q[0]\n",
        "\n",
        "              # And append to our training data\n",
        "        X.append(current_state)\n",
        "        y.append(current_qs)\n",
        "      \n",
        "      dqnagent.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnt5wXB7D7T2"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycKfYoOXvbH9"
      },
      "source": [
        "#Some initial values are declared here for the ease of changing\n",
        "\n",
        "\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.9975\n",
        "epsilon_min = 0.0001\n",
        "\n",
        "#An agent is created\n",
        "dqnagent = DQNAgent()\n",
        "\n",
        "#These are a few metrics to measure/track the performance at each episode\n",
        "steps = 0\n",
        "total_episodes = 0\n",
        "no_of_episodes_before_targetnetwork_update = 0\n",
        "\n",
        "#Here, we specifically solve for the 'A' variant of the puddle\n",
        "#A simple change in goal here adjusts the code to the user's choice\n",
        "puddle = PuddleEnv('A')\n",
        "puddle.assignStartState()\n",
        "temp_state = puddle.startState\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for episodes in range(5000):\n",
        "  #One episode is explained here below\n",
        "  print(\"epsilon\",epsilon)\n",
        "\n",
        "  puddle.assignStartState()\n",
        "  temp_state = puddle.startState\n",
        "  \n",
        "  steps = 0\n",
        "  end_bool = False\n",
        "  print(\"episode number\",episodes)\n",
        "\n",
        "  while end_bool == False and steps <= 100:\n",
        "    #Coding down the epsilon greedy concept\n",
        "\n",
        "    if random.random() < epsilon :\n",
        "      #Condition below accounts for exploration\n",
        "      temp_action = np.array(puddle.generalActions[random.randint(0,3)])\n",
        "      temp_transition = puddle.calculateTransition(temp_state,temp_action)\n",
        "      dqnagent.update_replay_memory(temp_transition)\n",
        "\n",
        "      steps += 1\n",
        "      temp_state = np.array(temp_transition[2]).flatten()\n",
        "      end_bool = np.array(temp_transition[4])[0]\n",
        "\n",
        "    else:\n",
        "      #Condition below accounts for exploitation\n",
        "      tempQValues = dqnagent.get_q(temp_state.reshape(1,1,2))\n",
        "      bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "      temp_action = np.array(puddle.generalActions[bestActionIndex])\n",
        "      temp_transition = puddle.calculateTransition(temp_state,temp_action) \n",
        "      dqnagent.update_replay_memory(temp_transition)\n",
        "\n",
        "      steps += 1\n",
        "      temp_state = np.array(temp_transition[2]).flatten()\n",
        "      end_bool = np.array(temp_transition[4])[0]\n",
        "\n",
        "  if epsilon > epsilon_min:\n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon = max(epsilon,epsilon_min)\n",
        "\n",
        "  print(\"steps\",steps)\n",
        "  \n",
        "\n",
        "  print(\"memorysize\",len(dqnagent.replay_memory))\n",
        "\n",
        "  \n",
        "\n",
        "  #Train DQN now\n",
        "  '''\n",
        "  At this point of the code, we collect mini batches of size 32 from replay \n",
        "  memory, use the target network to update targets for the main network to \n",
        "  learn from and train the main network.\n",
        "  A total of 10 batches are taken for each episode\n",
        "  '''\n",
        "  for batches in range(10):\n",
        "\n",
        "    #if replay memory is too small, we dont train and continue with more \n",
        "    #episodes for more data\n",
        "\n",
        "    if len(dqnagent.replay_memory) < 32:\n",
        "      break\n",
        "    # A mini batch of size 32 is taken \n",
        "    minibatch = random.sample(dqnagent.replay_memory, 32)\n",
        "\n",
        "    current_states = np.array([np.array(transition[0]).flatten() for transition in minibatch])\n",
        "    current_qs_list = dqnagent.model.predict(current_states.reshape(len(minibatch),1,2))\n",
        "    new_current_states = np.array([np.array(transition[2]).flatten() for transition in minibatch])\n",
        "    future_qs_list = dqnagent.target_model.predict(new_current_states.reshape(len(minibatch),1,2))\n",
        "\n",
        "    # empty arrays to populate for the network to learn from\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # Calculating the maximum q for the action from the target network\n",
        "    for index, (current_state, action, new_current_state,reward, done) in enumerate(minibatch):\n",
        "      if not [done]:\n",
        "        max_future_q = np.max(future_qs_list[index])\n",
        "        new_q = reward + DISCOUNT * max_future_q\n",
        "      else:\n",
        "        new_q = reward\n",
        "\n",
        "    # Update Q value for given state\n",
        "      current_qs = np.array(current_qs_list[index]).flatten()\n",
        "      for k in range(4):\n",
        "        comparison = puddle.generalActions[k] == action\n",
        "        if comparison.all():\n",
        "          action_index = k\n",
        "      current_qs[action_index] = new_q[0]\n",
        "\n",
        "     # And append to our training data\n",
        "      X.append(current_state)\n",
        "      y.append(current_qs)\n",
        "\n",
        "    #Finally the 'model' of the agent is trained\n",
        "    dqnagent.model.fit(np.array(X), np.array(y), batch_size=32, verbose=0, shuffle=False)\n",
        "\n",
        "  no_of_episodes_before_targetnetwork_update += 1\n",
        "  total_episodes += 1\n",
        "\n",
        "  # Check if '5' episodes are done so we can update our target network as well\n",
        "  if no_of_episodes_before_targetnetwork_update >= 5:\n",
        "    dqnagent.target_model.set_weights(dqnagent.model.get_weights())\n",
        "    no_of_episodes_before_targetnetwork_update = 0\n",
        "    print(\"Network target updated\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHgp_8AbvpHI"
      },
      "source": [
        "'''\n",
        "This snippet of the code, starts with the start state = [5,0]\n",
        "predicts the best action to take using the model and counts the steps it takes \n",
        "to reach the goal state 'A' = [0,11]\n",
        "'''\n",
        "\n",
        "sum = 0\n",
        "for i in range(20):\n",
        "  boolCompleted = [False]\n",
        "  stepCount = 0\n",
        "  tempState = np.array([5,0])\n",
        "  print('initial state',tempState)\n",
        "\n",
        "  while(boolCompleted == [False]):\n",
        "    tempQValues = dqnagent.get_q(tempState.reshape(1,1,2))\n",
        "    bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "    tempAction = np.array(puddle.generalActions[bestActionIndex])\n",
        "    tempTransition = puddle.calculateTransition(tempState,tempAction) \n",
        "    \n",
        "    boolCompleted = tempTransition[4]\n",
        "    stepCount += 1\n",
        "    tempState = np.array(tempTransition[2]).flatten()\n",
        "\n",
        "  print(stepCount)\n",
        "  print('---------------------')\n",
        "  sum += stepCount\n",
        "\n",
        "\n",
        "print('===========================================')\n",
        "print('Average of 20 runs',sum/20)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfU2t6Aq9TWL"
      },
      "source": [
        "'''\n",
        "This snippet of the code, starts with the start state = [6,0]\n",
        "predicts the best action to take using the model and counts the steps it takes \n",
        "to reach the goal state 'A' = [0,11]\n",
        "'''\n",
        "\n",
        "sum = 0\n",
        "for i in range(20):\n",
        "  boolCompleted = [False]\n",
        "  stepCount = 0\n",
        "  tempState = np.array([6,0])\n",
        "  print('initial state',tempState)\n",
        "\n",
        "  while(boolCompleted == [False]):\n",
        "    tempQValues = dqnagent.get_q(tempState.reshape(1,1,2))\n",
        "    bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "    tempAction = np.array(puddle.generalActions[bestActionIndex])\n",
        "    tempTransition = puddle.calculateTransition(tempState,tempAction) \n",
        "    \n",
        "    boolCompleted = tempTransition[4]\n",
        "    stepCount += 1\n",
        "    tempState = np.array(tempTransition[2]).flatten()\n",
        "\n",
        "  print(stepCount)\n",
        "  print('---------------------')\n",
        "  sum += stepCount\n",
        "\n",
        "\n",
        "print('===========================================')\n",
        "print('Average of 20 runs',sum/20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-G-mONI_KwN"
      },
      "source": [
        "'''\n",
        "This snippet of the code, starts with the start state = [10,0]\n",
        "predicts the best action to take using the model and counts the steps it takes \n",
        "to reach the goal state 'A' = [0,11]\n",
        "'''\n",
        "\n",
        "sum = 0\n",
        "for i in range(20):\n",
        "  boolCompleted = [False]\n",
        "  stepCount = 0\n",
        "  tempState = np.array([10,0])\n",
        "  print('initial state',tempState)\n",
        "\n",
        "  while(boolCompleted == [False]):\n",
        "    tempQValues = dqnagent.get_q(tempState.reshape(1,1,2))\n",
        "    bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "    tempAction = np.array(puddle.generalActions[bestActionIndex])\n",
        "    tempTransition = puddle.calculateTransition(tempState,tempAction) \n",
        "    \n",
        "    boolCompleted = tempTransition[4]\n",
        "    stepCount += 1\n",
        "    tempState = np.array(tempTransition[2]).flatten()\n",
        "\n",
        "  print(stepCount)\n",
        "  print('---------------------')\n",
        "  sum += stepCount\n",
        "\n",
        "\n",
        "print('===========================================')\n",
        "print('Average of 20 runs',sum/20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiT8G4lX_Nli"
      },
      "source": [
        "'''\n",
        "This snippet of the code, starts with the start state = [11,0]\n",
        "predicts the best action to take using the model and counts the steps it takes \n",
        "to reach the goal state 'A' = [0,11]\n",
        "'''\n",
        "\n",
        "\n",
        "sum = 0\n",
        "for i in range(20):\n",
        "  boolCompleted = [False]\n",
        "  stepCount = 0\n",
        "  tempState = np.array([11,0])\n",
        "  print('initial state',tempState)\n",
        "\n",
        "  while(boolCompleted == [False]):\n",
        "    tempQValues = dqnagent.get_q(tempState.reshape(1,1,2))\n",
        "    bestActionIndex = np.argmax(tempQValues.flatten())\n",
        "    tempAction = np.array(puddle.generalActions[bestActionIndex])\n",
        "    tempTransition = puddle.calculateTransition(tempState,tempAction) \n",
        "    \n",
        "    boolCompleted = tempTransition[4]\n",
        "    stepCount += 1\n",
        "    tempState = np.array(tempTransition[2]).flatten()\n",
        "\n",
        "  print(stepCount)\n",
        "  print('---------------------')\n",
        "  sum += stepCount\n",
        "\n",
        "\n",
        "print('===========================================')\n",
        "print('Average of 20 runs',sum/20)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}